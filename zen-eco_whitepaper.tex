\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{booktabs}
\usepackage{float}
\usepackage{geometry}
\geometry{margin=1in}

% Color definitions
\definecolor{zenblue}{RGB}{41,121,255}
\definecolor{zengreen}{RGB}{52,199,89}
\definecolor{zenorange}{RGB}{255,149,0}
\definecolor{codegray}{RGB}{245,245,245}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=zenblue,
    urlcolor=zenblue,
    citecolor=zenblue
}

% Code listing setup
\lstset{
    backgroundcolor=\color{codegray},
    basicstyle=\ttfamily\small,
    breaklines=true,
    captionpos=b,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray}
}

\title{
    \vspace{-2cm}
    \Large \textbf{Zen AI Model Family} \\
    \vspace{0.5cm}
    \Huge \textbf{Zen-Eco} \\
    \vspace{0.3cm}
    \large Consumer Hardware \\
    \vspace{0.5cm}
    \normalsize Technical Whitepaper v1.0
}

\author{
    Hanzo AI Research Team \\
    \texttt{research@hanzo.ai} \\
    \\
    Zoo Labs Foundation \\
    \texttt{foundation@zoolabs.org}
}

\date{September 2025}

\begin{document}

\maketitle

\begin{abstract}
We present \textbf{Zen-Eco}, a 4B parameter model optimized for consumer hardware. 
Built upon zen-3B, this model achieves state-of-the-art performance while maintaining exceptional efficiency 
with only 4B active parameters. Supporting 128K thinking tokens for advanced reasoning, the model represents a significant advancement in democratizing AI through sustainable and efficient architectures.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

The rapid advancement of artificial intelligence has created an unprecedented demand for models that balance capability with efficiency. 
\textbf{Zen-Eco} addresses this challenge by delivering enterprise-grade performance while maintaining a minimal computational footprint.

\subsection{Key Innovations}
\begin{itemize}
    \item \textbf{Efficient Architecture}: 4B active parameters from 4B total
    \item \textbf{Specialized Training}: Optimized for consumer hardware
    \item \textbf{Extended Context}: 32K context window
    \item \textbf{Thinking Mode}: 128K thinking tokens
    
    
\end{itemize}

\section{Architecture}

\subsection{Model Design}

Zen-Eco is based on the zen-3B architecture with several key modifications:

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Component} & \textbf{Specification} \\
\midrule
Total Parameters & 4B \\
Active Parameters & 4B \\
Base Model & zen-3B \\
Context Length & 32K \\
Thinking Tokens & 128K \\


Architecture Type & Transformer \\
\bottomrule
\end{tabular}
\caption{Zen-Eco Architecture Specifications}
\end{table}

\subsection{Technical Innovations}

\subsubsection{Mixture of Experts (MoE)}
The model uses a dense architecture with all parameters active during inference, optimized for maximum performance per parameter.

\subsubsection{Attention Mechanism}
Extended attention mechanisms support up to 32K context length with efficient KV-cache management.

\subsubsection{Thinking Mode}
Advanced reasoning through extended thinking tokens (up to 128K), enabling:
\begin{itemize}
    \item Step-by-step problem decomposition
    \item Self-correction and verification
    \item Complex multi-step reasoning
    \item Internal deliberation before response
\end{itemize}

\section{Performance Benchmarks}

\subsection{Evaluation Results}


\begin{table}[H]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Benchmark} & \textbf{Score} \\
\midrule
MMLU & 62.3\% \\
HumanEval & 35.2\% \\
GSM8K & 74.8\% \\
HellaSwag & 71.6\% \\
\bottomrule
\end{tabular}
\caption{Language Understanding Benchmarks}
\end{table}

\subsection{Efficiency Metrics}

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Inference Speed & 250 tokens/sec \\
Memory Usage (INT4) & 8 GB \\
Energy Efficiency & 95\% reduction \\
Latency (First Token) & 35 ms \\
\bottomrule
\end{tabular}
\caption{Efficiency Metrics}
\end{table}

\section{Training Methodology}

\subsection{Dataset}
The model was trained on a carefully curated dataset comprising:
\begin{itemize}
    \item High-quality filtered web data (2TB)
    \item Domain-specific corpora for consumer hardware
    \item Synthetic data generation for edge cases
    \item Human feedback through RLHF
\end{itemize}

\subsection{Training Process}
\begin{enumerate}
    \item \textbf{Pretraining}: 2 trillion tokens over 14 days on 8x A100
    \item \textbf{Supervised Fine-tuning}: Task-specific optimization
    \item \textbf{RLHF}: Alignment with human preferences
    \item \textbf{Constitutional AI}: Safety and helpfulness optimization
\end{enumerate}

\section{Use Cases and Applications}

\subsection{Primary Applications}
\item Conversational AI and chatbots
\item Content generation and summarization
\item Code completion and review
\item Educational assistance
\item Research and analysis

\subsection{Integration Examples}

\begin{lstlisting}[language=Python, caption=Basic Usage Example]
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load model and tokenizer
model = AutoModelForCausalLM.from_pretrained("zenlm/zen-eco-4b-instruct")
tokenizer = AutoTokenizer.from_pretrained("zenlm/zen-eco-4b-instruct")

# Generate response
inputs = tokenizer("Explain quantum computing", return_tensors="pt")
outputs = model.generate(**inputs, max_length=100)
response = tokenizer.decode(outputs[0])
\end{lstlisting}

\section{Environmental Impact}

\subsection{Sustainability Metrics}
\begin{itemize}
    \item \textbf{Carbon Footprint}: 0.05 kg COâ‚‚e per million inferences
    \item \textbf{Energy Usage}: 1.2 kWh per day (1000 users)
    \item \textbf{Efficiency Gain}: 95\% reduction vs comparable models
\end{itemize}

\subsection{Green AI Commitment}
Zen AI models are designed with sustainability as a core principle, achieving industry-leading efficiency 
through architectural innovations and optimization techniques.

\section{Safety and Alignment}

\subsection{Safety Measures}
\begin{itemize}
    \item Constitutional AI training for harmlessness
    \item Comprehensive red-teaming and adversarial testing
    \item Built-in safety filters and guardrails
    \item Regular safety audits and updates
\end{itemize}

\subsection{Ethical Considerations}
The model has been developed with careful attention to:
\begin{itemize}
    \item Bias mitigation through diverse training data
    \item Transparency in capabilities and limitations
    \item Privacy-preserving deployment options
    \item Responsible AI principles alignment
\end{itemize}

\section{Deployment Options}

\subsection{Available Formats}
\begin{itemize}
    \item \textbf{SafeTensors}: Original precision weights
    \item \textbf{GGUF}: Quantized formats (Q4\_K\_M, Q5\_K\_M, Q8\_0)
    \item \textbf{MLX}: Apple Silicon optimization (4-bit, 8-bit)
    \item \textbf{ONNX}: Cross-platform deployment (coming soon)
\end{itemize}

\subsection{Hardware Requirements}
\begin{table}[H]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Precision} & \textbf{Memory} & \textbf{Recommended Hardware} \\
\midrule
FP16 & 8 GB & RTX 3070 \\
INT8 & 4 GB & RTX 3060 \\
INT4 & 8 GB & M2 MacBook Air \\
\bottomrule
\end{tabular}
\caption{Hardware Requirements by Precision}
\end{table}

\section{Future Work}

\subsection{Planned Improvements}
\begin{itemize}
    \item Extended context windows (up to 1M tokens)
    \item Enhanced multimodal capabilities
    \item Improved efficiency through further optimization
    \item Expanded language support
\end{itemize}

\subsection{Research Directions}
\begin{itemize}
    \item Advanced reasoning mechanisms
    \item Self-supervised learning improvements
    \item Zero-shot generalization enhancement
    \item Continual learning capabilities
\end{itemize}

\section{Conclusion}

\textbf{Zen-Eco} represents a significant advancement in AI democratization, 
delivering exceptional performance for consumer hardware while maintaining 
unprecedented efficiency. Through innovative architecture design and careful optimization, 
the model achieves a balance between capability and sustainability that sets a new standard 
for responsible AI development.

\section*{Acknowledgments}

We thank the open-source community, our research partners, and the teams at Hanzo AI and 
Zoo Labs Foundation for their contributions to this work.

\bibliographystyle{plain}
\bibliography{references}

\appendix

\section{Model Card}

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Field} & \textbf{Value} \\
\midrule
Model Name & Zen-Eco \\
Version & 1.0.0 \\
Release Date & September 2025 \\
License & Apache 2.0 \\
Repository & \href{https://huggingface.co/zenlm/zen-eco-4b-instruct}{huggingface.co/zenlm/zen-eco-4b-instruct} \\
Documentation & \href{https://github.com/zenlm/zen}{github.com/zenlm/zen} \\
Contact & research@hanzo.ai \\
\bottomrule
\end{tabular}
\caption{Model Card Information}
\end{table}

\end{document}